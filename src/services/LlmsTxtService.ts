import { DatabaseService } from './DatabaseService';

/**
 * LLMs.txt æœå‹™
 * è² è²¬æŠ“å–å’Œè™•ç† llms.txt å…§å®¹
 */
export class LlmsTxtService {
  private static cache: {
    url: string;
    content: string;
    chunks: string[];
    timestamp: number;
  } | null = null;

  private static readonly CACHE_DURATION = 3600000; // 1 å°æ™‚
  private static readonly CHUNK_SIZE = 500; // æ¯å€‹ chunk çš„å­—ç¬¦æ•¸
  private static readonly CHUNK_OVERLAP = 100; // chunk ä¹‹é–“çš„é‡ç–Šå­—ç¬¦æ•¸

  /**
   * ç²å–ä¸¦è™•ç† llms.txt å…§å®¹
   */
  static async getLlmsTxtChunks(): Promise<string[]> {
    try {
      // å¾è³‡æ–™åº«ç²å– URL
      const url = await DatabaseService.getSetting('llms_txt_url');
      if (!url) {
        console.log('No llms.txt URL configured');
        return [];
      }

      // æª¢æŸ¥ç·©å­˜
      const now = Date.now();
      if (this.cache && this.cache.url === url && (now - this.cache.timestamp) < this.CACHE_DURATION) {
        console.log('Using cached llms.txt chunks');
        return this.cache.chunks;
      }

      // æŠ“å–å…§å®¹
      console.log('Fetching llms.txt from:', url);
      const response = await fetch(url);
      if (!response.ok) {
        throw new Error(`Failed to fetch llms.txt: ${response.status}`);
      }

      const content = await response.text();
      console.log('Fetched llms.txt content, length:', content.length);

      // åˆ‡åˆ†æˆ chunks
      const chunks = this.splitIntoChunks(content);
      console.log('Split into', chunks.length, 'chunks');

      // æ›´æ–°ç·©å­˜
      this.cache = {
        url,
        content,
        chunks,
        timestamp: now
      };

      return chunks;
    } catch (error) {
      console.error('Error fetching llms.txt:', error);
      return [];
    }
  }

  /**
   * å°‡æ–‡æœ¬åˆ‡åˆ†æˆ chunksï¼ˆå¸¶é‡ç–Šï¼‰
   */
  private static splitIntoChunks(text: string): string[] {
    const chunks: string[] = [];
    let start = 0;

    while (start < text.length) {
      const end = Math.min(start + this.CHUNK_SIZE, text.length);
      const chunk = text.substring(start, end);
      chunks.push(chunk);

      // ç§»å‹•åˆ°ä¸‹ä¸€å€‹ chunkï¼Œè€ƒæ…®é‡ç–Š
      start += this.CHUNK_SIZE - this.CHUNK_OVERLAP;
    }

    return chunks;
  }

  /**
   * æå–ä¸­æ–‡å­—ç¬¦å’Œè‹±æ–‡å–®è©ä½œç‚ºé—œéµå­—
   */
  private static extractKeywords(text: string): string[] {
    const textLower = text.toLowerCase();
    // æå–ä¸­æ–‡å­—ç¬¦ï¼ˆå–®å€‹å­—ç¬¦ï¼‰
    const chineseChars = textLower.match(/[\u4e00-\u9fa5]/g) || [];
    // æå–è‹±æ–‡å–®è©ï¼ˆ2å€‹å­—ç¬¦ä»¥ä¸Šï¼‰
    const englishWords = textLower.match(/[a-z]{2,}/g) || [];
    // æå–æ•¸å­—
    const numbers = textLower.match(/\d+/g) || [];
    return [...chineseChars, ...englishWords, ...numbers];
  }

  /**
   * è¨ˆç®— BM25 åˆ†æ•¸
   */
  private static calculateBM25Score(
    queryKeywords: string[],
    docKeywords: string[],
    avgDocLength: number,
    k1: number = 1.5,
    b: number = 0.75
  ): number {
    if (docKeywords.length === 0) return 0;

    let score = 0;
    const docLength = docKeywords.length;

    for (const queryKeyword of queryKeywords) {
      // è¨ˆç®—è©é » (term frequency)
      const tf = docKeywords.filter(k => k === queryKeyword).length;
      if (tf === 0) continue;

      // BM25 å…¬å¼
      const numerator = tf * (k1 + 1);
      const denominator = tf + k1 * (1 - b + b * (docLength / avgDocLength));
      score += numerator / denominator;
    }

    return score;
  }

  /**
   * æœç´¢ç›¸é—œçš„ chunksï¼ˆä½¿ç”¨ BM25 ç®—æ³•ï¼‰
   */
  static async searchChunks(query: string): Promise<Array<{ chunk: string; context: string; score: number }>> {
    const chunks = await this.getLlmsTxtChunks();
    if (chunks.length === 0) {
      return [];
    }

    console.log('ğŸ” LlmsTxtService.searchChunks() called with query:', query);

    // æå–æŸ¥è©¢é—œéµå­—
    const queryKeywords = this.extractKeywords(query);
    console.log('ğŸ” Query keywords:', queryKeywords);

    // è¨ˆç®—å¹³å‡æ–‡æª”é•·åº¦
    const allChunkKeywords = chunks.map(chunk => this.extractKeywords(chunk));
    const avgChunkLength = allChunkKeywords.reduce((sum, keywords) => sum + keywords.length, 0) / allChunkKeywords.length;

    // è¨ˆç®—æ¯å€‹ chunk çš„åˆ†æ•¸
    const scoredChunks = chunks.map((chunk, index) => {
      const chunkKeywords = allChunkKeywords[index];
      const bm25Score = this.calculateBM25Score(queryKeywords, chunkKeywords, avgChunkLength);

      return {
        chunk,
        context: '',
        score: bm25Score,
        index
      };
    });

    // éæ¿¾å‡ºåˆ†æ•¸ > 0 çš„çµæœä¸¦æ’åº
    const results = scoredChunks
      .filter(item => item.score > 0)
      .sort((a, b) => b.score - a.score);

    console.log('ğŸ” LlmsTxtService found', results.length, 'matching chunks');
    if (results.length > 0) {
      console.log('ğŸ” Top chunk score:', results[0].score.toFixed(2));
    }

    // å–å‰ 5 å€‹çµæœï¼Œä¸¦æ·»åŠ å‰å¾Œæ–‡
    const topResults = results.slice(0, 5);

    // ç‚ºæ¯å€‹çµæœæ·»åŠ å‰å¾Œæ–‡ï¼ˆå‰ä¸€å€‹å’Œå¾Œä¸€å€‹ chunkï¼‰
    topResults.forEach(result => {
      const contextChunks: string[] = [];

      // æ·»åŠ å‰ä¸€å€‹ chunk
      if (result.index > 0) {
        contextChunks.push(chunks[result.index - 1]);
      }

      // æ·»åŠ ç•¶å‰ chunk
      contextChunks.push(result.chunk);

      // æ·»åŠ å¾Œä¸€å€‹ chunk
      if (result.index < chunks.length - 1) {
        contextChunks.push(chunks[result.index + 1]);
      }

      result.context = contextChunks.join('\n...\n');
    });

    return topResults.map(r => ({ chunk: r.chunk, context: r.context, score: r.score }));
  }

  /**
   * æ¸…é™¤ç·©å­˜
   */
  static clearCache(): void {
    this.cache = null;
  }
}

